{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf8eba4",
   "metadata": {},
   "source": [
    "### similarity of 2 vectors\n",
    "$$ similarity = cos(\\theta) = \\frac{w_1w_2}{\\parallel w_1 \\parallel \\parallel w_2 \\parallel} $$\n",
    "\n",
    "### Word2vec\n",
    "\n",
    "![similarity](./imgs/similarity.png)\n",
    "\n",
    "[leimao's blog](https://leimao.github.io/article/Word2Vec-Classic/)\n",
    "\n",
    "[skip-gram nathanrooy](https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/)\n",
    "\n",
    "[cbow explain](https://www.youtube.com/watch?v=akRbuXokLSo&t=1195s)\n",
    "\n",
    "![word2vec](word2vec.png)\n",
    "\n",
    "Both `CBOW` and `Skip-Gram` does not have `hidden-layer`\n",
    "\n",
    "Trainable `embedding matrix` $E \\in R^{nxv}$ `n (number of word)` `v (dimension of vector)`, all your word will be here in this `embedding matrix`\n",
    "\n",
    "### CBOW\n",
    "\n",
    "![cbow](./imgs/cbow.png)\n",
    "\n",
    "Weight matrix $W \\in R^{nxd}$ , and bias $b \\in R^{n}$ and four words $v_{t-2}$,$v_{t-1}$,$v_{t+1}$,$v_{t+2}$, average vector input $v^{'}_{t} = \\frac{1}{4} (v_{t-2} + v_{t-1} + v_{t+1} + v_{t+2})$\n",
    "\n",
    "$$o_t = \\frac{1}{4} [(W v_{t-2} + b)+(W v_{t-1} + b)+(W v_{t+1} + b)+(W v_{t+2} + b)]$$\n",
    "$$= \\frac{1}{4} (W v_{t-2} + W v_{t-1} + W v_{t+1} + W v_{t+2}) + b$$\n",
    "$$= \\frac{1}{4} W (v_{t-2} + v_{t-1} + v_{t+1} + v_{t+2}) + b$$\n",
    "$$= W v^{'}_{t} + b$$\n",
    "\n",
    "![cbow2](./imgs/cbow2.png)\n",
    "\n",
    "$$h = U^{T} w^{c}$$\n",
    "$$w^{t} = V^{T} h$$\n",
    "$$o_t = softmax(w^{t})$$\n",
    "\n",
    "example \n",
    "\n",
    "$$ o^{t}_{1} = \\frac{e^{w^{t}_1}}{ e^{w^{t}_1} + e^{w^{t}_2} + e^{w^{t}_3} + e^{w^{t}_4} + e^{w^{t}_5} + e^{w^{t}_6} }$$\n",
    "$$ o^{t}_{2} = \\frac{e^{w^{t}_2}}{ e^{w^{t}_1} + e^{w^{t}_2} + e^{w^{t}_3} + e^{w^{t}_4} + e^{w^{t}_5} + e^{w^{t}_6} }$$\n",
    "\n",
    "likelihood function\n",
    "\n",
    "$$ \\Pi^{T}_{t=1} = P(w^{t}|w^{t-m},w^{t-m+1},...,w^{t-1},w^{t+1},...,w^{t+m-1},w^{t+m}) $$\n",
    "\n",
    "example\n",
    "\n",
    "`The` dog is digging bone --> P(\"The\"|\"dog\",\"is\",\"digging\",\"bone\")\n",
    "\n",
    "The `dog` is digging bone --> P(\"dog\"|\"The\",\"is\",\"digging\",\"bone\")\n",
    "\n",
    "The dog `is` digging bone --> P(\"is\"|\"The\",\"dog\",\"digging\",\"bone\")\n",
    "\n",
    "...\n",
    "\n",
    "maximum likelihood estimation of the `CBOW` model is equivalent to minimizing the loss function\n",
    "\n",
    "$$ J(U,V) = -\\sum^{T}_{t=1} log P(w^{t}|w^{t-m},w^{t-m+1},...,w^{t-1},w^{t+1},...,w^{t+m-1},w^{t+m}) $$\n",
    "\n",
    "\n",
    "- $w^{t}_{i}$ Predicted word (Scalar)\n",
    "- $w^{c}_{i}$ Context word (Scalar)\n",
    "- $U$ Input-Hidden Weights Matrix\n",
    "- $V$ Hidden-Output Weights Matrix\n",
    "- $T$ length of text sequence\n",
    "- $t$ timestep\n",
    "- $m$ window size\n",
    "\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "![skip-gram](./imgs/skip-gram.png)\n",
    "\n",
    "`Skip-gram` take one input word and predict the past and the future words, example: take one word \"playing\" and predict \"I\",\"love\",\"computer\",\"game\", this can be misleading, so in pratice we only use one word to predict one word.\n",
    "\n",
    "$$ o_{t-2} = W v_{t} + b $$\n",
    "$$ o_{t-1} = W v_{t} + b $$\n",
    "$$ o_{t+1} = W v_{t} + b $$\n",
    "$$ o_{t+2} = W v_{t} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d43a5f",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] [word2vec math](https://leimao.github.io/article/Word2Vec-Classic/)\n",
    "\n",
    "[2] [cbow from scratch](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html)\n",
    "\n",
    "[3] [skipgram from scratch](http://mbenhaddou.com/2019/12/14/word2vec-concept-from-scratch-part-2/)\n",
    "\n",
    "[4] [word2vec-numpy](https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/)\n",
    "\n",
    "[5] [word2vec](https://github.com/nickvdw/word2vec-from-scratch/blob/master/word2vec.ipynb)\n",
    "\n",
    "[6] [Word2vec - Mô hình hóa từ bằng mạng học sâu](https://www.youtube.com/watch?v=akRbuXokLSo)\n",
    "\n",
    "[7] [Xây dựng mô hình phân loại cảm xúc và trích xuất embedding của từ (Train Sentiment Classification)](https://www.youtube.com/watch?v=JIafLwlGzBA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
