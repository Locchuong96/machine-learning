{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f669318",
   "metadata": {},
   "source": [
    "### sorted by the frquency of words\n",
    "\n",
    "- from tensorflow\n",
    "- from scartch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bde415",
   "metadata": {},
   "source": [
    "**from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bea8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96519230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'image',\n",
       " 'image_dataset_from_directory',\n",
       " 'sequence',\n",
       " 'text',\n",
       " 'text_dataset_from_directory',\n",
       " 'timeseries_dataset_from_array']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de86bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your text corpus\n",
    "sentences = [\n",
    "    'Hello, this is a beautiful day',\n",
    "    'It is hard to live on Ukraine these day',\n",
    "    'The war in Ukraine must stop!',\n",
    "    'The dog is digging',\n",
    "    'She is sining the \"Moon River\" song'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "793c06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100 # the maximum number of words to keep, base on word frequency\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' # filter all these character\n",
    "lower = True # option to convert the word to be key\n",
    "split = ' ' # split the word by space\n",
    "oov_token = \"<OOV>\" # out of vocabulary\n",
    "# init your custom tokenizer\n",
    "tokenizer = prep.text.Tokenizer(num_words=num_words,\n",
    "                                filters= filters,\n",
    "                                lower=lower,\n",
    "                                split=split,\n",
    "                                oov_token=oov_token\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6a71b",
   "metadata": {},
   "source": [
    "there are still have a problem with `\"` and `'`, example `It's`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "03cc3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit word sentences\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "38c5a967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " 'char_level',\n",
       " 'document_count',\n",
       " 'filters',\n",
       " 'fit_on_sequences',\n",
       " 'fit_on_texts',\n",
       " 'get_config',\n",
       " 'index_docs',\n",
       " 'index_word',\n",
       " 'lower',\n",
       " 'num_words',\n",
       " 'oov_token',\n",
       " 'sequences_to_matrix',\n",
       " 'sequences_to_texts',\n",
       " 'sequences_to_texts_generator',\n",
       " 'split',\n",
       " 'texts_to_matrix',\n",
       " 'texts_to_sequences',\n",
       " 'texts_to_sequences_generator',\n",
       " 'to_json',\n",
       " 'word_counts',\n",
       " 'word_docs',\n",
       " 'word_index']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be180067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'day': 4,\n",
       " 'ukraine': 5,\n",
       " 'hello': 6,\n",
       " 'this': 7,\n",
       " 'a': 8,\n",
       " 'beautiful': 9,\n",
       " 'it': 10,\n",
       " 'hard': 11,\n",
       " 'to': 12,\n",
       " 'live': 13,\n",
       " 'on': 14,\n",
       " 'these': 15,\n",
       " 'war': 16,\n",
       " 'in': 17,\n",
       " 'must': 18,\n",
       " 'stop': 19,\n",
       " 'dog': 20,\n",
       " 'digging': 21,\n",
       " 'she': 22,\n",
       " 'sining': 23,\n",
       " 'moon': 24,\n",
       " 'river': 25,\n",
       " 'song': 26}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the word index\n",
    "word_index = tokenizer.word_index\n",
    "print(type(word_index))\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "87bd0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = [\n",
    "    'The fire burned down the house',\n",
    "    'Tom is a cat and Jerry is a mouse',\n",
    "    'Please keep the umbrella with you, it is raining',\n",
    "    'Take a note everything is not good',\n",
    "    'Flower is the good, leaf is great'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0ddd270d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 1, 1, 3, 1],\n",
       " [1, 2, 8, 1, 1, 1, 2, 8, 1],\n",
       " [1, 1, 3, 1, 1, 1, 10, 2, 1],\n",
       " [1, 8, 1, 1, 2, 1, 1],\n",
       " [1, 2, 3, 1, 1, 2, 1]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "new_sequences # 1 meaning out of volcabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3f3b60c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 9, 7, 7]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list = [len(s) for s in new_sequences]\n",
    "len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21889f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in new_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0c5f23e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  3,  1,  1,  1,  3,  1],\n",
       "       [ 1,  2,  8,  1,  1,  1,  2,  8,  1],\n",
       "       [ 1,  1,  3,  1,  1,  1, 10,  2,  1],\n",
       "       [ 0,  0,  1,  8,  1,  1,  2,  1,  1],\n",
       "       [ 0,  0,  1,  2,  3,  1,  1,  2,  1]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the sentences become 1 length base on the longest setence and add new padding\n",
    "padding_sequences = prep.sequence.pad_sequences(new_sequences,maxlen = max([len(s) for s in new_sequences]))\n",
    "padding_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526b5fd",
   "metadata": {},
   "source": [
    "You can also run the `terminal code` on jupyter notebook just for reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90c05cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm_Headlines_Dataset.json\n",
      "Sarcasm_Headlines_Dataset_v2.json\n"
     ]
    }
   ],
   "source": [
    "# inspect the sarcasm dataset\n",
    "!ls ./sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d7a43af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "879efa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file):\n",
    "    for l in open(file,'r'):\n",
    "        yield json.loads(l) # generate each line in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "94b977b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object parse_data at 0x0000000030BE83C0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = parse_data('./sarcasm/Sarcasm_Headlines_Dataset.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4ab289cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26709\n",
      "[{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}]\n"
     ]
    }
   ],
   "source": [
    "data = list(data)\n",
    "print(len(data))\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ba0bc768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28619\n",
      "[{'is_sarcastic': 1, 'headline': 'thirtysomething scientists unveil doomsday clock of hair loss', 'article_link': 'https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205'}]\n"
     ]
    }
   ],
   "source": [
    "data2 = parse_data('./sarcasm/Sarcasm_Headlines_Dataset_v2.json')\n",
    "data2 = list(data2)\n",
    "print(len(data2))\n",
    "print(data2[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b15d75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "51147337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26709/26709 [00:00<00:00, 476920.08it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels  = []\n",
    "\n",
    "for i in tqdm(data):\n",
    "    #print(i['headline'])\n",
    "    sentences.append(i['headline'])\n",
    "    labels.append(i['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e5f7c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = prep.text.Tokenizer(num_words = max([len(x) for x in sentences]),\n",
    "                                filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                lower = True,\n",
    "                                split = ' ',\n",
    "                                oov_token = \"<OOV>\"\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e8dcf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a900a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "aa84058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e8571147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26709, 254)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  6,  1,  1],\n",
       "       [ 0,  0,  0, ...,  1,  9,  1],\n",
       "       [ 0,  0,  0, ..., 46,  2,  1],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  1,  9, 68],\n",
       "       [ 0,  0,  0, ...,  1,  1,  1],\n",
       "       [ 0,  0,  0, ...,  4,  1,  1]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding\n",
    "padded_sequences = prep.sequence.pad_sequences(sequences,maxlen =  max([len(x) for x in sentences]))\n",
    "print(padded_sequences.shape)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68312aa",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "[1] [tokenization in NLP](https://www.youtube.com/watch?v=FT1ZZdcur5A&t=869s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
